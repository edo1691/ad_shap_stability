{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "123c1709-aadd-4c93-a80a-0a0c44290983",
   "metadata": {},
   "source": [
    "# iForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2520ef24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11e4854b-87bd-490e-88e4-9a64631dc5bb",
   "metadata": {
    "tags": []
   },
   "source": [
    "## General libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "feba20a1-5d20-4127-888b-b0e4c72f4757",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from os.path import join\n",
    "import json\n",
    "import datetime\n",
    "\n",
    "import shap\n",
    "from shap_selection import feature_selection\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de633751",
   "metadata": {},
   "source": [
    "### Load enviroment variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "64a57d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv('../.env')\n",
    "\n",
    "code_root = os.environ['CODE_ROOT']\n",
    "cfg_path = os.environ['CFG_PATH']\n",
    "data_root = os.environ['DATA_ROOT']\n",
    "\n",
    "sys.path.insert(0, code_root)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d1ee99b-97d7-4059-b242-380656087497",
   "metadata": {},
   "source": [
    "### Specific libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b670745e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.load.functions import get_fs_dataset, fs_datasets_hyperparams\n",
    "from src.plots.functions import plot_3d_surface, plot_2d_surface, boxplot_stability, lineplot_stability\n",
    "from src.utils.functions import add_custom_repeating_sequence, add_sequence_to_dataframe, prepare_subsets\n",
    "from src.optimization.functions import opt_value\n",
    "\n",
    "# if we want to create an iteractive plot or not\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "%matplotlib widget"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a16e2a",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "761a8dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from matplotlib.patches import Patch\n",
    "\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def process_data(dataset_id, df):\n",
    "    \"\"\"\n",
    "    Processes SHAP data files by reading them, filtering, and calculating additional metrics including the interquartile range (IQR).\n",
    "    \n",
    "    Parameters:\n",
    "    - dataset_id: The unique identifier for the dataset.\n",
    "    - data_root: The root directory where the data files are located.\n",
    "    \n",
    "    Returns:\n",
    "    - df: The processed pandas DataFrame containing selected variables and calculated metrics.\n",
    "    \"\"\"\n",
    "\n",
    "    # Filter dataframe\n",
    "    df = df[(df.n_estimators >= 25) & (df.n_estimators <= 300)]\n",
    "    \n",
    "    # Calculate percentiles, stability index, and IQR\n",
    "    df['shap_q1'] = df['stab_shap'].apply(lambda x: np.percentile(x, 25))  # Lower quartile (25th percentile)\n",
    "    df['shap_q2'] = df['stab_shap'].apply(lambda x: np.percentile(x, 50))  # Median (50th percentile)\n",
    "    df['shap_q3'] = df['stab_shap'].apply(lambda x: np.percentile(x, 75))  # Upper quartile (75th percentile)\n",
    "    df['stability index'] = df['stab_shap'].apply(lambda x: np.mean(x))\n",
    "    df['IQR'] = df['stab_shap'].apply(lambda x: np.percentile(x, 75) - np.percentile(x, 25))  # Interquartile range (Q3 - Q1)\n",
    "    df['dataset'] = dataset_id\n",
    "    df['hpo'] = np.where((df.n_estimators == 100) & (df.max_feats == df.max_feats.max()),\n",
    "        'Benchmark',\n",
    "        'Our model')\n",
    "\n",
    "    # Select variables to keep\n",
    "    df.rename(columns={'f1-score': 'f1_score'}, inplace=True)\n",
    "    \n",
    "    var = ['dataset', 'hpo', 'n_estimators', 'max_feats', 'n_feats', \n",
    "           'f1_score', 'precision', 'recall', 'stab_model', 'stab_shap', \n",
    "           'shap_q1', 'shap_q2', 'shap_q3', 'stability index', 'IQR', 'roc_auc']\n",
    "    df = df[var]\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def filter_and_select(df, fi_shap_all, boxplot=False, factor=0.99):\n",
    "    \"\"\"\n",
    "    Filters and selects rows from a DataFrame based on specified conditions.\n",
    "    \n",
    "    Parameters:\n",
    "    - df: A pandas DataFrame containing model evaluation metrics and parameters.\n",
    "    \n",
    "    Returns:\n",
    "    - df_filtered: A filtered DataFrame based on the conditions.\n",
    "    \"\"\"\n",
    "    # Define the variables of interest\n",
    "    #var = ['dataset', 'hpo', 'n_estimators', 'n_feats', 'max_feats', 'roc_auc', 'precision', 'recall', 'f1_median', 'stability index', 'shap_stab']\n",
    "    #df = df[var]\n",
    "\n",
    "    #filter_shap = int(fi_shap_all[fi_shap_all.cum_value_percentage<99.9].n_feats.tail(1))\n",
    "\n",
    "    df1_1 = df[df.hpo == 'Benchmark'].head(1)\n",
    "    # Extract specific metrics from df1_1\n",
    "    roc = df1_1.roc_auc.iloc[0] * factor\n",
    "    precision = df1_1.precision.iloc[0] * factor\n",
    "    recall = df1_1.recall.iloc[0] * factor\n",
    "    f1_score = df1_1['f1_score'].iloc[0] * factor\n",
    "    stability = df1_1['stability index'].iloc[0]\n",
    "    IQR = df1_1['IQR'].iloc[0]\n",
    "    n_features = df1_1.n_feats.iloc[0]\n",
    "\n",
    "    # baseline = {'f1_score': f1_score, 'precision': precision, 'recall': recall, 'roc_auc': roc}\n",
    "    baseline = {'precision': precision, 'roc_auc': roc}\n",
    "    df1_2 = df[df.hpo != 'Benchmark']\n",
    "    #criteria = np.where(df1_1.n_feats>=25, False, True)\n",
    "    df1_2 = df1_2[df1_2['stability index'] >= stability]\n",
    "    if n_features >= 10:\n",
    "        df1_2 = df1_2[(df1_2.n_feats < n_features)]\n",
    "    df1_2 = find_best_row_by_distance_and_stability(df1_2, \n",
    "                                                    baseline, \n",
    "                                                    stability_col='stability index',\n",
    "                                                    IQR_col='IQR',\n",
    "                                                    IQR_base=IQR,\n",
    "                                                    distance_weight=1, \n",
    "                                                    stability_weight=0.3, \n",
    "                                                    threshold=np.inf)\n",
    "\n",
    "    # Extract specific feature numbers\n",
    "    dataframes = [df1_1, df1_2]\n",
    "    df = pd.concat(dataframes)\n",
    "    #df = df.drop_duplicates()\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def find_best_row_by_distance_and_stability(df, baseline, stability_col='stability', IQR_col='IQR', IQR_base=np.inf, distance_weight=1, stability_weight=0.5, threshold=np.inf):\n",
    "    \"\"\"\n",
    "    Finds the best matching row in df based on a combination of closeness to the baseline values, the stability score, IQR filtering,\n",
    "    and optionally preferring rows with values higher than the baseline.\n",
    "    \n",
    "    Parameters:\n",
    "    - df: DataFrame to search within.\n",
    "    - baseline: Dictionary specifying baseline values for columns, excluding the stability and IQR columns.\n",
    "    - stability_col: Name of the column containing stability scores.\n",
    "    - IQR_col: Name of the column containing IQR values.\n",
    "    - IQR_base: The maximum allowed IQR value for selection.\n",
    "    - distance_weight: Weight of the distance score in the final score calculation.\n",
    "    - stability_weight: Weight of the stability score in the final score calculation.\n",
    "    - threshold: Maximum allowed final score to consider a row close enough.\n",
    "    \n",
    "    Returns:\n",
    "    - A single-row DataFrame if a suitable row is found; otherwise, None.\n",
    "    \"\"\"\n",
    "    if df.empty:\n",
    "        return None\n",
    "\n",
    "    # Check for missing columns\n",
    "    missing_cols = [col for col in baseline.keys() if col not in df.columns]\n",
    "    if missing_cols:\n",
    "        raise ValueError(f\"Missing columns in DataFrame: {missing_cols}\")\n",
    "    if stability_col not in df.columns or IQR_col not in df.columns:\n",
    "        raise ValueError(f\"Missing required columns in DataFrame\")\n",
    "\n",
    "    # First, try to filter rows with IQR <= IQR_base\n",
    "    df_IQR_filtered = df[df[IQR_col] <= IQR_base]\n",
    "\n",
    "    # If no rows fulfill the IQR requirement\n",
    "    if df_IQR_filtered.empty:\n",
    "        df_IQR_filtered = df\n",
    "        baseline['IQR'] = IQR_base\n",
    "\n",
    "    # First, try to filter rows with metrics >= baseline\n",
    "    for metric, baseline_value in baseline.items():\n",
    "        df_baseline_filtered = df_IQR_filtered[df_IQR_filtered[metric] >= baseline_value]\n",
    "    \n",
    "    # If no rows fulfill the metrics >= baseline requirement\n",
    "    if df_baseline_filtered.empty:\n",
    "        df_baseline_filtered = df_IQR_filtered\n",
    "    \n",
    "    # Compute the \"distance\" for each row in the filtered DataFrame\n",
    "    distance = df_baseline_filtered.apply(lambda row: np.sqrt(sum((row[baseline.keys()] - pd.Series(baseline))**2)), axis=1)\n",
    "\n",
    "    # Normalize distance and stability scores to [0, 1]\n",
    "    distance_norm = (distance - distance.min()) / (distance.max() - distance.min())\n",
    "    stability_norm = (df_baseline_filtered[stability_col] - df_baseline_filtered[stability_col].min()) / (df_baseline_filtered[stability_col].max() - df_baseline_filtered[stability_col].min())\n",
    "\n",
    "    # Calculate combined score\n",
    "    final_score = distance_weight * distance_norm - stability_weight * stability_norm\n",
    "\n",
    "    # Find the row with the minimum final score within the threshold\n",
    "    best_idx = final_score.idxmin()\n",
    "\n",
    "    if final_score[best_idx] <= threshold:\n",
    "        return df_baseline_filtered.loc[[best_idx]]\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "def process_graph_data(df):\n",
    "    \"\"\"\n",
    "    Processes SHAP data files by reading them, filtering, and calculating additional metrics.\n",
    "    \n",
    "    Parameters:\n",
    "    - dataset_id: The unique identifier for the dataset.\n",
    "    - data_root: The root directory where the data files are located.\n",
    "    \n",
    "    Returns:\n",
    "    - df: The processed pandas DataFrame containing selected variables and calculated metrics.\n",
    "    \"\"\"\n",
    "    # Find the index of the maximum 'n_feat' in each 'n_estimators' and 'n_feats' group\n",
    "    idx = df.groupby(['n_estimators', 'n_feats'])['max_feats'].idxmax()\n",
    "    # Filter the DataFrame to keep only the rows with the maximum 'n_feat' per group\n",
    "    df = df.loc[idx].reset_index(drop=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def append_dataframes(dataframes):\n",
    "    \"\"\"\n",
    "    Appends a list of DataFrames into a single DataFrame.\n",
    "    \n",
    "    Parameters:\n",
    "    - dataframes: List of pandas DataFrames with the same format.\n",
    "    \n",
    "    Returns:\n",
    "    - A single DataFrame containing all rows from the input DataFrames.\n",
    "    \"\"\"\n",
    "    # Concatenate the DataFrames row-wise\n",
    "    combined_df = pd.concat(dataframes, ignore_index=True)\n",
    "    return combined_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae171188",
   "metadata": {},
   "source": [
    "## Visualization of the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "252e9b32-4841-43ed-abc8-6f46c4990022",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Users/eduardosepulveda/workspace_github/ad_shap_stability/test/data/outputs/creditcard_shap.parquet'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 22\u001b[0m\n\u001b[1;32m     19\u001b[0m path_fi_shap \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(data_root, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutputs\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_fi_shap\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Read the parquet files\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_parquet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_shap\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m fi_shap_all \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_parquet(path_fi_shap)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# Process the data\u001b[39;00m\n",
      "File \u001b[0;32m~/workspace_github/ad_shap_stability/venv/lib/python3.9/site-packages/pandas/io/parquet.py:493\u001b[0m, in \u001b[0;36mread_parquet\u001b[0;34m(path, engine, columns, storage_options, use_nullable_dtypes, **kwargs)\u001b[0m\n\u001b[1;32m    446\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    447\u001b[0m \u001b[38;5;124;03mLoad a parquet object from the file path, returning a DataFrame.\u001b[39;00m\n\u001b[1;32m    448\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    489\u001b[0m \u001b[38;5;124;03mDataFrame\u001b[39;00m\n\u001b[1;32m    490\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    491\u001b[0m impl \u001b[38;5;241m=\u001b[39m get_engine(engine)\n\u001b[0;32m--> 493\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimpl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    495\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    496\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    497\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_nullable_dtypes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_nullable_dtypes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    498\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    499\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/workspace_github/ad_shap_stability/venv/lib/python3.9/site-packages/pandas/io/parquet.py:233\u001b[0m, in \u001b[0;36mPyArrowImpl.read\u001b[0;34m(self, path, columns, use_nullable_dtypes, storage_options, **kwargs)\u001b[0m\n\u001b[1;32m    230\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m manager \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marray\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    231\u001b[0m     to_pandas_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msplit_blocks\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m  \u001b[38;5;66;03m# type: ignore[assignment]\u001b[39;00m\n\u001b[0;32m--> 233\u001b[0m path_or_handle, handles, kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfilesystem\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43m_get_path_or_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    234\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    235\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfilesystem\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    236\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    237\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    238\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    239\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    240\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapi\u001b[38;5;241m.\u001b[39mparquet\u001b[38;5;241m.\u001b[39mread_table(\n\u001b[1;32m    241\u001b[0m         path_or_handle, columns\u001b[38;5;241m=\u001b[39mcolumns, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    242\u001b[0m     )\u001b[38;5;241m.\u001b[39mto_pandas(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mto_pandas_kwargs)\n",
      "File \u001b[0;32m~/workspace_github/ad_shap_stability/venv/lib/python3.9/site-packages/pandas/io/parquet.py:102\u001b[0m, in \u001b[0;36m_get_path_or_handle\u001b[0;34m(path, fs, storage_options, mode, is_dir)\u001b[0m\n\u001b[1;32m     92\u001b[0m handles \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m     94\u001b[0m     \u001b[38;5;129;01mnot\u001b[39;00m fs\n\u001b[1;32m     95\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_dir\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[38;5;66;03m# fsspec resources can also point to directories\u001b[39;00m\n\u001b[1;32m    101\u001b[0m     \u001b[38;5;66;03m# this branch is used for example when reading from non-fsspec URLs\u001b[39;00m\n\u001b[0;32m--> 102\u001b[0m     handles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    103\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath_or_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\n\u001b[1;32m    104\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    105\u001b[0m     fs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    106\u001b[0m     path_or_handle \u001b[38;5;241m=\u001b[39m handles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/workspace_github/ad_shap_stability/venv/lib/python3.9/site-packages/pandas/io/common.py:798\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    789\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[1;32m    790\u001b[0m             handle,\n\u001b[1;32m    791\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    794\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    795\u001b[0m         )\n\u001b[1;32m    796\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    797\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m--> 798\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    799\u001b[0m     handles\u001b[38;5;241m.\u001b[39mappend(handle)\n\u001b[1;32m    801\u001b[0m \u001b[38;5;66;03m# Convert BytesIO or file objects passed with an encoding\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Users/eduardosepulveda/workspace_github/ad_shap_stability/test/data/outputs/creditcard_shap.parquet'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming 'data_root', 'process_data', 'append_dataframes', 'filter_and_select', \n",
    "# and 'process_graph_data' are defined elsewhere in your code.\n",
    "\n",
    "# List of dataset identifiers\n",
    "dataset_ids = ['arrhythmia', 'creditcard', 'musk', 'cardio', 'bank', 'mammography']\n",
    "\n",
    "# Initialize lists to store dataframes\n",
    "processed_dataframes = []\n",
    "filtered_dataframes = []\n",
    "graph_dataframes = []\n",
    "\n",
    "# Loop through each dataset ID\n",
    "for dataset_id in dataset_ids:\n",
    "    # Construct file paths for SHAP values and feature importances\n",
    "    path_shap = os.path.join(data_root, \"outputs\", f\"{dataset_id}_shap.parquet\")\n",
    "    path_fi_shap = os.path.join(data_root, \"outputs\", f\"{dataset_id}_fi_shap\")\n",
    "    \n",
    "    # Read the parquet files\n",
    "    df = pd.read_parquet(path_shap)\n",
    "    fi_shap_all = pd.read_parquet(path_fi_shap)\n",
    "\n",
    "    # Process the data\n",
    "    processed_df = process_data(dataset_id, df)\n",
    "    processed_dataframes.append(processed_df)\n",
    "    \n",
    "    # Filter and select data\n",
    "    filtered_df = filter_and_select(processed_df, fi_shap_all, factor=1.0)\n",
    "    filtered_dataframes.append(filtered_df)\n",
    "    \n",
    "    # Prepare graph data\n",
    "    graph_df = process_graph_data(processed_df)\n",
    "    graph_dataframes.append(graph_df)\n",
    "\n",
    "# Append processed dataframes into a single dataframe\n",
    "df = append_dataframes(processed_dataframes)\n",
    "\n",
    "# Append filtered dataframes into a single dataframe for the group analysis\n",
    "df_group = append_dataframes(filtered_dataframes)\n",
    "var = ['dataset', 'hpo', 'n_estimators', 'max_feats', 'n_feats', 'roc_auc', 'precision', 'recall', 'f1_score',\n",
    "       'stab_model', 'stability index', 'shap_q1', 'shap_q2', 'shap_q3', 'IQR', 'stab_shap']\n",
    "df_group = df_group[var].round(4)\n",
    "\n",
    "# Perform an inner join based on 'dataset', 'n_estimators', 'max_feats', and 'n_feats'\n",
    "var = ['dataset', 'max_feats', 'n_feats']\n",
    "df_benchmark = pd.merge(df, df_group[var][df_group.hpo=='Benchmark'], on=var, how='inner')\n",
    "df_benchmark['hpo'] = 'Benchmark'\n",
    "df_our_model = pd.merge(df, df_group[var][df_group.hpo=='Our model'], on=var, how='inner')\n",
    "df_our_model['hpo'] = 'Our model'\n",
    "df = df_benchmark.append(df_our_model, ignore_index=True)\n",
    "\n",
    "\n",
    "# The graph dataframes are already prepared in the loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afaf7f07-4bd9-4411-a31b-5c9060935d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_group[['dataset', 'hpo', 'n_estimators', 'max_feats', 'n_feats', 'roc_auc', 'precision', 'recall', 'f1_score', 'stab_model', 'stability index', 'shap_q1', 'shap_q2', 'shap_q3', 'IQR']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ebf652-8ca3-411e-b5df-6db5e2eb74bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "image_1 =  '../boxplots.png'\n",
    "boxplot_stability(df_group, size=1)\n",
    "plt.savefig(image_1, bbox_inches='tight', pad_inches=0.1)  #Save the plot to a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "846138c4-70a6-4b28-ade7-3108d30ba87c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = df[(df.hpo=='Benchmark')] #((df.dataset=='arrhythmia') | (df.dataset=='creditcard')) & \n",
    "test[['dataset', 'hpo', 'max_feats', 'n_feats', 'n_estimators', 'stab_shap']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "950a446c-e0a2-42c9-a16f-4adaae697d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import ast\n",
    "\n",
    "# Datos de ejemplo en formato de texto para ser convertidos a un DataFrame\n",
    "# Convertir el diccionario a un DataFrame\n",
    "df = pd.DataFrame(test)\n",
    "\n",
    "# Extraer los datos y las etiquetas\n",
    "data = [df.loc[df['n_estimators'] == n, 'stab_shap'].values[0] for n in df['n_estimators']]\n",
    "labels = [f'n_estimators={n}' for n in df['n_estimators']]\n",
    "\n",
    "# Crear el gráfico de box-plots\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.boxplot(data, patch_artist=True, notch=True, vert=True)\n",
    "\n",
    "# Añadir etiquetas a los ejes\n",
    "plt.xticks(range(1, len(labels) + 1), labels, rotation=45)\n",
    "plt.ylabel('stab_shap values')\n",
    "plt.title('Box-Plots por n_estimators')\n",
    "\n",
    "# Mostrar el gráfico\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c998e635-003f-4db1-a42a-0035259a7951",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Convertir el diccionario a un DataFrame\n",
    "df = pd.DataFrame(test)\n",
    "\n",
    "# Obtener la lista de datasets únicos\n",
    "datasets = df['dataset'].unique()\n",
    "\n",
    "# Crear la figura con subplots 2x3\n",
    "fig, axs = plt.subplots(3, 2, figsize=(9, 8), sharex=True)\n",
    "\n",
    "# Iterar sobre cada dataset y crear un subplot\n",
    "for i, dataset in enumerate(datasets):\n",
    "    row, col = divmod(i, 2)\n",
    "    ax = axs[row, col]\n",
    "    subset = df[df['dataset'] == dataset]\n",
    "    data = [subset.loc[subset['n_estimators'] == n, 'stab_shap'].values[0] for n in subset['n_estimators']]\n",
    "    labels = subset['n_estimators'].unique()\n",
    "    \n",
    "    ax.boxplot(data, patch_artist=False, notch=False, vert=True)\n",
    "    ax.set_xticks(range(1, len(labels) + 1))\n",
    "    ax.set_xticklabels(labels)\n",
    "    # ax.set_ylim(0, 1)  # Establecer los límites del eje Y de 0 a 1\n",
    "    # ax.set_ylabel(f'stability measure')\n",
    "    ax.set_title(f'{dataset}')\n",
    "\n",
    "# Eliminar los subplots vacíos si hay menos de 6 datasets\n",
    "if len(datasets) < 6:\n",
    "    for j in range(len(datasets), 6):\n",
    "        fig.delaxes(axs.flatten()[j])\n",
    "\n",
    "# Añadir etiqueta al eje X\n",
    "# Ajustar el layout para evitar solapamientos\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bcac863-dba9-4aaa-9723-e783d593870f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def boxplot_stability(df, ax, fontsize_title=8, fontsize_axes=6, stability_feat='stability index', boxplot_feat='stab_shap'):\n",
    "    \"\"\"\n",
    "    Creates a boxplot where the X-axis is the number of estimators and the Y-axis is the stability index,\n",
    "    with a boxplot of the specified feature for each value of n_estimators.\n",
    "\n",
    "    Parameters:\n",
    "    - df: DataFrame containing the columns 'n_estimators', 'stability index', and the feature to be plotted as boxplot.\n",
    "    - ax: The matplotlib axes object where the plot will be drawn.\n",
    "    - fontsize_title: Font size for the title.\n",
    "    - fontsize_axes: Font size for the axes labels.\n",
    "    - stability_feat: The feature to be plotted on the Y-axis.\n",
    "    - boxplot_feat: The feature to be plotted as boxplot for each value of n_estimators.\n",
    "    \"\"\"\n",
    "    # Ensure 'n_estimators' and 'stab_shap' are of correct data types\n",
    "    df['n_estimators'] = df['n_estimators'].astype(str)\n",
    "    df[boxplot_feat] = pd.to_numeric(df[boxplot_feat], errors='coerce')\n",
    "\n",
    "    # Drop any rows with NaN values in 'stab_shap'\n",
    "    df = df.dropna(subset=[boxplot_feat])\n",
    "\n",
    "    # Create the boxplot\n",
    "    sns.boxplot(x='n_estimators', y=boxplot_feat, data=df, ax=ax)\n",
    "\n",
    "    # Set axis labels and title\n",
    "    ax.set_xlabel('Number of Estimators', fontsize=fontsize_axes)\n",
    "    ax.set_ylabel(stability_feat.capitalize(), fontsize=fontsize_axes)\n",
    "    ax.set_title('Stability Index by Number of Estimators', fontsize=fontsize_title)\n",
    "\n",
    "    # Set tick parameters\n",
    "    ax.tick_params(axis='x', labelsize=fontsize_axes)\n",
    "    ax.tick_params(axis='y', labelsize=fontsize_axes)\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a13b96d-77dc-4e96-bce2-ef4d6fd542e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "boxplot_stability(test, ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ebed211-dd1f-4b38-ab5e-f7e02f6fce3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 25))  # Overall figure size\n",
    "\n",
    "# Assuming col and row represent the number of columns and rows of subplots you want\n",
    "col, row = 6, 1\n",
    "\n",
    "# Loop through each dataset_id to create a subplot for each\n",
    "for i, dataset_id in enumerate(dataset_ids, start=1):\n",
    "    ax = plt.subplot(col, row, i)\n",
    "    # Filter df_all for the current dataset_id and plot\n",
    "    filtered_df = df[df.dataset == dataset_id]\n",
    "    lineplot_stability(filtered_df, ax, primary_feat='f1_score', secondary_feat='stability index')\n",
    "\n",
    "plt.tight_layout()  # Adjust layout to prevent overlap\n",
    "plt.subplots_adjust(wspace=0.5, hspace=0.3)  # Adjust the spacing as needed\n",
    "\n",
    "# Save the plot to a file\n",
    "image_2 = '../comparison.png'\n",
    "plt.savefig(image_2, bbox_inches='tight', pad_inches=0.1)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76347c76-86a4-4b5c-bb47-2026a1f32f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e88dc8f6-8354-4f4b-8247-c9a6849635b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 25))  # Overall figure size\n",
    "\n",
    "# Assuming col and row represent the number of columns and rows of subplots you want\n",
    "col, row = 6, 1\n",
    "\n",
    "# Loop through each dataset_id to create a subplot for each\n",
    "for i, dataset_id in enumerate(dataset_ids, start=1):\n",
    "    ax1 = plt.subplot(col, row, i)\n",
    "    # Filter df_all for the current dataset_id and plot\n",
    "    filtered_df = df_all[df_all.dataset == dataset_id]\n",
    "\n",
    "    # Primary plot (e.g., precision)\n",
    "    ax1.plot(filtered_df['x'], filtered_df['precision'], color='b', label='Precision')\n",
    "    ax1.set_ylabel('Precision', color='b')\n",
    "    ax1.tick_params(axis='y', labelcolor='b')\n",
    "\n",
    "    # Create secondary y-axis\n",
    "    ax2 = ax1.twinx()\n",
    "    # Secondary plot (e.g., stability index)\n",
    "    ax2.plot(filtered_df['x'], filtered_df['stability index'], color='r', label='Stability Index')\n",
    "    ax2.set_ylabel('Stability Index', color='r')\n",
    "    ax2.tick_params(axis='y', labelcolor='r')\n",
    "\n",
    "    # Optionally set titles or legends\n",
    "    ax1.set_title(f'Dataset ID: {dataset_id}')\n",
    "    ax1.set_xlabel('X-axis label')  # Set the x-axis label if needed\n",
    "\n",
    "plt.tight_layout()  # Adjust layout to prevent overlap\n",
    "plt.subplots_adjust(wspace=0.5, hspace=1)  # Adjust the spacing as needed\n",
    "\n",
    "# Save the plot to a file\n",
    "image_2 = '../comparison.png'\n",
    "plt.savefig(image_2, bbox_inches='tight', pad_inches=0.1)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e733d19-f538-41c0-b1d5-c36f20e7a6b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "plt.figure(figsize=(10, 18))  # Overall figure size\n",
    "\n",
    "col, row = 6, 1  # Number of columns and rows for subplots\n",
    "\n",
    "for i, dataset_id in enumerate(dataset_ids, start=1):\n",
    "    ax = plt.subplot(col, row, i)\n",
    "    \n",
    "    # Filter the DataFrame for the current dataset and model\n",
    "    current_df = df[(df.dataset == dataset_id) & (df.hpo == 'Our model')]\n",
    "    \n",
    "    # Assuming 'stab_shap' is what you want to boxplot\n",
    "    ax.boxplot(current_df['stab_shap'])\n",
    "    \n",
    "    # Set y-axis from 0 to 1\n",
    "    # ax.set_ylim([0.3, 1])\n",
    "    \n",
    "    # Set x-axis labels to n_estimators values\n",
    "    # This assumes you have the same number of n_estimators for each dataset_id. \n",
    "    # If not, you might need to adjust this part.\n",
    "    ax.set_xticklabels(current_df['n_estimators'].unique(), rotation=0, ha=\"center\")\n",
    "    \n",
    "    # Add a small title for each subplot\n",
    "    ax.set_title(f\"Dataset: {dataset_id}\", fontsize=10)\n",
    "    \n",
    "    # Optional: Set labels for clarity\n",
    "    ax.set_xlabel(\"n_estimators\")\n",
    "    ax.set_ylabel(\"Stability\")\n",
    "\n",
    "plt.tight_layout()  # Adjust layout to prevent overlap\n",
    "plt.subplots_adjust(wspace=0.1, hspace=0.9)  # Adjust the spacing as needed\n",
    "\n",
    "# Save the plot to a file\n",
    "image_2 = '../comparison_boxplots.png'\n",
    "plt.savefig(image_2, bbox_inches='tight', pad_inches=0.1)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02fb7a52-6cfa-4b26-8ee1-91eeb2116a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(dataset_id, df):\n",
    "    \"\"\"\n",
    "    Processes SHAP data files by reading them, filtering, and calculating additional metrics including the interquartile range (IQR).\n",
    "    \n",
    "    Parameters:\n",
    "    - dataset_id: The unique identifier for the dataset.\n",
    "    - data_root: The root directory where the data files are located.\n",
    "    \n",
    "    Returns:\n",
    "    - df: The processed pandas DataFrame containing selected variables and calculated metrics.\n",
    "    \"\"\"\n",
    "\n",
    "    # Filter dataframe\n",
    "    df = df[(df.n_estimators >= 25) & (df.n_estimators <= 300)]\n",
    "    \n",
    "    # Calculate percentiles, stability index, and IQR\n",
    "    df['shap_q1'] = df['stab_shap'].apply(lambda x: np.percentile(x, 25))  # Lower quartile (25th percentile)\n",
    "    df['shap_q2'] = df['stab_shap'].apply(lambda x: np.percentile(x, 50))  # Median (50th percentile)\n",
    "    df['shap_q3'] = df['stab_shap'].apply(lambda x: np.percentile(x, 75))  # Upper quartile (75th percentile)\n",
    "    df['stability index'] = df['stab_shap'].apply(lambda x: np.mean(x))\n",
    "    df['IQR'] = df['stab_shap'].apply(lambda x: np.percentile(x, 75) - np.percentile(x, 25))  # Interquartile range (Q3 - Q1)\n",
    "    df['dataset'] = dataset_id\n",
    "    df['hpo'] = np.where((df.n_estimators == 100) & (df.max_feats == df.max_feats.max()),\n",
    "        'Benchmark',\n",
    "        'Our model')\n",
    "\n",
    "    # Select variables to keep\n",
    "    df.rename(columns={'f1-score': 'f1_score'}, inplace=True)\n",
    "    \n",
    "    var = ['dataset', 'hpo', 'n_estimators', 'max_feats', 'n_feats', \n",
    "           'f1_score', 'precision', 'recall', 'stab_model', 'stab_shap', \n",
    "           'shap_q1', 'shap_q2', 'shap_q3', 'stability index', 'IQR', 'roc_auc']\n",
    "    df = df[var]\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5696db33-2ead-4f8e-85dc-1da56cd36c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_id = 'allianz'\n",
    "#df_names = [dataset_id1, dataset_id2, dataset_id3, dataset_id4, dataset_id5, dataset_id6]\n",
    "\n",
    "path_shap = os.path.join(data_root, \"outputs\", f\"{dataset_id}_shap.parquet\")\n",
    "path_fi_shap = os.path.join(data_root, \"outputs\", f\"{dataset_id}_fi_shap.parquet\")\n",
    "# Read data\n",
    "df = pd.read_parquet(path_shap)\n",
    "fi_shap_all = pd.read_parquet(path_fi_shap)  # Note: fi_shap_all is read but not used in the provided code snippet\n",
    "\n",
    "df_real = process_data(dataset_id, df)\n",
    "\n",
    "factor = 1.0\n",
    "#df_fil_real = filter_and_select(df_real, factor)\n",
    "\n",
    "var = ['dataset', 'hpo', 'n_estimators', 'max_feats', 'n_feats', 'stability index', 'stab_shap', 'shap_q2', 'stab_model', 'IQR']\n",
    "#df_group = round(df_fil_real[var],4)\n",
    "\n",
    "df_graph_real = process_graph_data(df_real)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4635f0b0-e2cd-48bc-acbc-6aa6ca133878",
   "metadata": {},
   "outputs": [],
   "source": [
    "fi_shap_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1fc64e5-e542-420a-a913-509a2af6f2a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2345dcd6-c43d-44e0-8385-804e96debb0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "group_1 = df_real[var][(df_real.n_estimators==100) & \n",
    "                       (df_real.n_feats==df_real.n_feats.max()) & \n",
    "                       (df_real.max_feats==df_real.max_feats.max())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb3a2564-c87f-4769-b41a-c79dda2beb83",
   "metadata": {},
   "outputs": [],
   "source": [
    "group_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7574b23c-177d-4541-8514-4f4ef4bee379",
   "metadata": {},
   "outputs": [],
   "source": [
    "group_2 = df_real[var][(df_real.n_feats==34)].sort_values('stability index', ascending=False).head(1)\n",
    "group_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3638e1b9-960b-45f0-8998-237f31f52e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_group = append_dataframes([group_1, group_2])\n",
    "df_group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f205c3-b1d0-49cd-84a6-2e1b195e467c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "image_1 =  '../boxplots_real.png'\n",
    "boxplot_stability(df_group, size=4)\n",
    "#plt.savefig(image_1, bbox_inches='tight', pad_inches=0.1)  #Save the plot to a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddfdd7aa-881a-48a5-b1c8-c416c6680d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "566a620a-e46d-4dd7-9447-6d39769986b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 20))  # Overall figure size\n",
    "\n",
    "# First dataset\n",
    "ax1 = plt.subplot(3, 2, 1, projection='3d')\n",
    "plot_3d_surface(df1, 'n_estimators', 'n_feats', 'precision', ax1, fontsize_title=10, fontsize_axes=6, cmap='YlGnBu', x_step=25, y_step=50, opt_color='red', title=f'{dataset_id1}', alpha=0.85, edgecolor=None)\n",
    "ax2 = plt.subplot(3, 2, 2, projection='3d')\n",
    "plot_3d_surface(df1, 'n_estimators', 'n_feats', 'stability index', ax2, fontsize_title=10, fontsize_axes=6, cmap='YlGnBu', x_step=25, y_step=50, opt_color='red', title=f'{dataset_id1}', alpha=.9, edgecolor=None)\n",
    "\n",
    "# Second dataset\n",
    "ax1 = plt.subplot(3, 2, 3, projection='3d')\n",
    "plot_3d_surface(df2, 'n_estimators', 'n_feats', 'precision', ax1, fontsize_title=10, fontsize_axes=6, cmap='YlGnBu', x_step=25, y_step=5, opt_color='red', title=f'{dataset_id2}', alpha=0.85, edgecolor=None)\n",
    "ax2 = plt.subplot(3, 2, 4, projection='3d')\n",
    "plot_3d_surface(df2, 'n_estimators', 'n_feats', 'stability index', ax2, fontsize_title=10, fontsize_axes=6, cmap='YlGnBu', x_step=25, y_step=5, opt_color='red', title=f'{dataset_id2}', alpha=.9, edgecolor=None)\n",
    "\n",
    "# Third dataset\n",
    "ax1 = plt.subplot(3, 2, 5, projection='3d')\n",
    "plot_3d_surface(df3, 'n_estimators', 'n_feats', 'precision', ax1, fontsize_title=10, fontsize_axes=6, cmap='YlGnBu', x_step=25, y_step=50, opt_color='red', title=f'{dataset_id3}', alpha=0.85, edgecolor=None)\n",
    "ax2 = plt.subplot(3, 2, 6, projection='3d')\n",
    "plot_3d_surface(df3, 'n_estimators', 'n_feats', 'stability index', ax2, fontsize_title=10, fontsize_axes=6, cmap='YlGnBu', x_step=25, y_step=50, opt_color='red', title=f'{dataset_id3}', alpha=.9, edgecolor=None)\n",
    "\n",
    "plt.tight_layout()  # Adjust layout\n",
    "\n",
    "\n",
    "# Adjust the spacing between the plots\n",
    "plt.subplots_adjust(wspace=0.1, hspace=0.1)  # Adjust the width space as needed for better visualization\n",
    "\n",
    "#plt.savefig(image_1, bbox_inches='tight', pad_inches=0.3)  # Save the plot to a file\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "common-cpu.m80",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m80"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "c7bb90c4ec68b2a8968b0075ab0b1cb7a78770acf7a7acf2e36e903fa05bac64"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

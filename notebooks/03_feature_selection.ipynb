{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "123c1709-aadd-4c93-a80a-0a0c44290983",
   "metadata": {},
   "source": [
    "# iForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2520ef24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11e4854b-87bd-490e-88e4-9a64631dc5bb",
   "metadata": {
    "tags": []
   },
   "source": [
    "## General libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "feba20a1-5d20-4127-888b-b0e4c72f4757",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from os.path import join\n",
    "import json\n",
    "import datetime\n",
    "\n",
    "import shap\n",
    "\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.metrics import f1_score, roc_curve, auc, confusion_matrix, classification_report, precision_recall_curve\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "#from pyod.models.iforest import IForest as IsolationForest\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de633751",
   "metadata": {},
   "source": [
    "### Load enviroment variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "64a57d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv('../.env')\n",
    "\n",
    "code_root = os.environ['CODE_ROOT']\n",
    "cfg_path = os.environ['CFG_PATH']\n",
    "data_root = os.environ['DATA_ROOT']\n",
    "\n",
    "sys.path.insert(0, code_root)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d1ee99b-97d7-4059-b242-380656087497",
   "metadata": {},
   "source": [
    "### Specific libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b670745e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.load.functions import get_fs_dataset, fs_datasets_hyperparams\n",
    "\n",
    "from src.model.functions import train_and_evaluate_iforest\n",
    "#from src.stability.functions import stability_measure_model, stability_measure_shap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c360966",
   "metadata": {},
   "source": [
    "## General parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "27ef64e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iter_fs = 1\n",
    "n_iter = 1\n",
    "contamination_percentage = [0.8] \n",
    "trees = [25, 50, 100]\n",
    "\n",
    "# Function to calculate median of a list\n",
    "def calculate_median(numbers_list):\n",
    "    return np.median(numbers_list)\n",
    "\n",
    "# Function to calculate mean of a list\n",
    "def calculate_mean(numbers_list):\n",
    "    return np.mean(numbers_list)\n",
    "\n",
    "# Define aggregation criteria for each variable\n",
    "aggregation_rules = {\n",
    "    'n_iter': 'max',\n",
    "    'n_iter_fs': 'max',\n",
    "    'f1_median': 'mean',\n",
    "    'recall_median': 'mean',\n",
    "    'precision_median': 'mean',\n",
    "    'roc_auc': 'mean',\n",
    "    'iforest_stab_unif_median': 'median',\n",
    "    'shap_stab_median': 'median',\n",
    "    'shap_stab_mean': 'mean',\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7db27ee-405e-4faa-a7e9-6b8cdf41d437",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Arrhythmia"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70f383d5",
   "metadata": {},
   "source": [
    "**Dataset source**: http://odds.cs.stonybrook.edu/arrhythmia-dataset/ (data is transformed from .mat to .csv format)\n",
    "\n",
    "Shebuti Rayana (2016). ODDS Library [http://odds.cs.stonybrook.edu]. Stony Brook, NY: Stony Brook University, Department of Computer Science.\n",
    "\n",
    "**Additional sources**:\n",
    "\n",
    "Liu, Fei Tony, Kai Ming Ting, and Zhi-Hua Zhou. “Isolation forest.” 2008 Eighth IEEE International Conference on Data Mining. IEEE, 2008.\n",
    "\n",
    "K. M. Ting, J. T. S. Chuan, and F. T. Liu. “Mass: A New Ranking Measure for Anomaly Detection.“, IEEE Transactions on Knowledge and Data Engineering, 2009.\n",
    "\n",
    "F. Keller, E. Muller, K. Bohm.“HiCS: High-contrast subspaces for density-based outlier ranking.” ICDE, 2012."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4a527899-7fe8-4aea-80f8-2e0447fde772",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_id = 'arrhythmia'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "97f1d076",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = get_fs_dataset(dataset_id, data_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "76666a27-50e4-4926-aec3-bc7d0303f482",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyper = fs_datasets_hyperparams(dataset_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7a470d6c-c28d-4798-aec0-2361530d8bf8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(452, 275)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "937686a7-6d97-4ed0-890a-4575bd4403f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Col1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>y</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>66</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Col1\n",
       "y      \n",
       "0   386\n",
       "1    66"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.pivot_table(data,\n",
    "             values = 'Col1',\n",
    "               index = 'y', \n",
    "              aggfunc = 'count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6a540a60-dc06-477f-acda-ac0d564fb5ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "excluded_cols = ['Col15', 'Col63', 'Col65', 'Col79', 'Col127', 'Col128','Col135', 'Col137', 'Col139','Col141','Col147', 'Col152', 'Col153', 'Col160', 'Col200', 'Col260', 'Col270']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72d0f5e9-900d-4121-9d91-3b201911e14f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### iForest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a16e2a",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "761a8dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "train_data = data.copy()\n",
    "\n",
    "path_if = os.path.join(data_root, \"outputs\", f\"{dataset_id}_results_if.parquet\")\n",
    "path_fs_shap = os.path.join(data_root, \"outputs\", f\"{dataset_id}_results_fs_shap.csv\")\n",
    "path_fi_shap = os.path.join(data_root, \"outputs\", f\"{dataset_id}_results_fi_shap.csv\")\n",
    "path_shap = os.path.join(data_root, \"outputs\", f\"{dataset_id}_results_shap.parquet\")\n",
    "path_fs_diffi = os.path.join(data_root, \"outputs\", f\"{dataset_id}_results_fs_diffi.csv\")\n",
    "path_fi_diffi = os.path.join(data_root, \"outputs\", f\"{dataset_id}_results_fi_diffi.csv\")\n",
    "path_diffi = os.path.join(data_root, \"outputs\", f\"{dataset_id}_results_diffi.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67bd69a7",
   "metadata": {},
   "source": [
    "### Iforest full features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5bb22a04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'contamination': 0.146, 'max_samples': 256, 'n_estimators': 100}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hyper = fs_datasets_hyperparams(dataset_id)\n",
    "hyper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e2789843",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration by tree number: 25\n",
      "  Iteration by contamination: 0.117\n",
      "    Number of featured: 257\n",
      "Iteration by tree number: 50\n",
      "  Iteration by contamination: 0.117\n",
      "    Number of featured: 257\n",
      "Iteration by tree number: 100\n",
      "  Iteration by contamination: 0.117\n",
      "    Number of featured: 257\n",
      "Duration: 0:00:10.701899\n"
     ]
    }
   ],
   "source": [
    "# Capture the start time\n",
    "start_time = datetime.datetime.now()\n",
    "\n",
    "results_if = train_and_evaluate_iforest(train_data, dataset_id=dataset_id, hyper=hyper, n_tree_estimators=trees, contamination_percentage=contamination_percentage, excluded_cols=excluded_cols, n_iter_fs=n_iter_fs, n_iter=n_iter)\n",
    "\n",
    "# Capture the finish time\n",
    "finish_time = datetime.datetime.now()\n",
    "\n",
    "# Calculate the duration\n",
    "duration = finish_time - start_time\n",
    "\n",
    "print(f\"Duration: {duration}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf7871ad-99cf-4d5e-8753-1c6e0a163b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_if"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a65a408b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over each column in ranked_df (each column represent an event)\n",
    "for col in ranked_df.columns:\n",
    "    sorted_ixs = np.array(np.argsort(ranked_df[col]))  # Sorting based on the current column\n",
    "\n",
    "    # Initialize point_rankings for the current column\n",
    "    point_rankings_col = np.zeros_like(point_rankings)\n",
    "\n",
    "    for ii, si in enumerate(sorted_ixs):\n",
    "        point_rankings_col[si, i] = ii + 1\n",
    "\n",
    "    # normalize rankings\n",
    "    point_rankings_col = point_rankings_col / nte  # lower rank = more normal\n",
    "\n",
    "    # compute the stability score for multiple iterations\n",
    "    random_stdev_col = np.sqrt((nte + 1) * (nte - 1) / (12 * nte ** 2))\n",
    "    stability_scores_col = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da63e304",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_if"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ae271e",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_if.shap_iforest_stab_unif_median[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fae9e9b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the function to the 'shap_iforest_stab_unif_median' column\n",
    "results_if['shap_stab_median'] = results_if['shap_iforest_stab_unif_median'].apply(calculate_median)\n",
    "results_if['shap_stab_mean'] = results_if['shap_iforest_stab_unif_median'].apply(calculate_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62d5b9f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_if_group = results_if.groupby(['n_feats', 'n_estimators', 'contamination']).agg(aggregation_rules)\n",
    "results_if_group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1164eadb",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_results = pd.read_parquet(path_if)\n",
    "data_results.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd8d80e",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_without_fs = list(results_if_group.f1_median)[0]\n",
    "stability_without_fs = list(results_if_group.iforest_stab_unif_median)[0]\n",
    "stability_shap_without_fs = list(results_if_group.shap_stab_median)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c80d140",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_if_group.to_parquet(path_if)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acdf93ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "fs_shap, fi_shap, _ = fs_iforest_with_shap(train_data, contamination_percentage=contamination_percentage, excluded_cols=excluded_cols, n_iter_fs=n_iter_fs, n_iter=n_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e174ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "fi_shap[fi_shap.value > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d616499b",
   "metadata": {},
   "outputs": [],
   "source": [
    "select_var = list(fi_shap[fi_shap.value > 0].feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f0428f",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_if = train_and_evaluate_iforest(train_data[select_var + ['y']], dataset_id=dataset_id, n_tree_estimators=trees, contamination_percentage=contamination_percentage, excluded_cols=excluded_cols, n_iter_fs=n_iter_fs, n_iter=n_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "270d94d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the function to the 'shap_iforest_stab_unif_median' column\n",
    "results_if['shap_stab_median'] = results_if['shap_iforest_stab_unif_median'].apply(calculate_median)\n",
    "results_if['shap_stab_mean'] = results_if['shap_iforest_stab_unif_median'].apply(calculate_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a364762",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_if_group = results_if.groupby(['n_feats', 'n_estimators', 'contamination']).agg(aggregation_rules)\n",
    "results_if_group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a25d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_if_group.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0157bde2",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_without_fs = list(results_if_group.f1_median)[0]\n",
    "stability_without_fs = list(results_if_group.iforest_stab_unif_median)[0]\n",
    "stability_shap_without_fs = list(results_if_group.shap_stab_median)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae0c43ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_if = os.path.join(data_root, \"outputs\", f\"{dataset_id}_results_if_selected_var.parquet\")\n",
    "results_if_group.to_parquet(path_if)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae171188",
   "metadata": {},
   "source": [
    "## SHAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68267356",
   "metadata": {},
   "outputs": [],
   "source": [
    "fs_shap, fi_shap, _ = fs_iforest_with_shap(train_data, contamination_percentage=contamination_percentage, excluded_cols=excluded_cols, n_iter_fs=n_iter_fs, n_iter=n_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51916fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "fi_shap_all = process_fi(fi_shap, group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c3a23e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "fi_shap_all.to_parquet(path_fi_shap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9de21c3e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results_shap = train_and_evaluate_iforest(train_data, dataset_id, fi_shap_all, n_tree_estimators=trees, contamination_percentage=contamination_percentage, excluded_cols=excluded_cols, n_iter_fs=n_iter_fs, n_iter=n_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fba59dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the function to the 'shap_iforest_stab_unif_median' column\n",
    "results_shap['shap_stab_median'] = results_shap['shap_iforest_stab_unif_median'].apply(calculate_median)\n",
    "results_shap['shap_stab_mean'] = results_shap['shap_iforest_stab_unif_median'].apply(calculate_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad4efa31",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_shap_group = results_shap.groupby(['n_feats', 'n_estimators', 'contamination']).agg(aggregation_rules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bb98127",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_shap_group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18df311d",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_shap_group.to_parquet(path_shap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c44d5dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(path_shap)\n",
    "df = df.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2278f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.n_feats==254]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b625303",
   "metadata": {},
   "source": [
    "# DIFFI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "230690d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "fs_diffi, fi_diffi, avg_f1_diffi = fs_iforest_with_diffi(train_data, contamination_percentage=contamination_percentage, excluded_cols=excluded_cols, n_iter_fs=n_iter_fs, n_iter=n_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b632985",
   "metadata": {},
   "outputs": [],
   "source": [
    "fi_diffi_all = process_fi(fi_diffi, group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4a20b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fi_diffi_all.to_parquet(path_fi_diffi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed694b7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results_diffi = train_and_evaluate_iforest(train_data, dataset_id, fi_diffi_all, n_tree_estimators=trees, contamination_percentage=contamination_percentage, excluded_cols=excluded_cols, n_iter_fs=n_iter_fs, n_iter=n_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f02c8074",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_diffi_group = results_diffi.groupby(['n_feats', 'n_estimators', 'contamination']).agg(aggregation_rules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0971b503",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_diffi_group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0f738fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_diffi_group.to_parquet(path_diffi)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8ba1477",
   "metadata": {},
   "source": [
    "### Graph Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d2f0730",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def add_percentage_annotations(ax, x_values, y_values, percentage_values):\n",
    "    for i, txt in enumerate(percentage_values):\n",
    "        ax.annotate(f'{round(txt, 3)}%', (x_values.iloc[i], y_values.iloc[i]),\n",
    "                    textcoords=\"offset points\", xytext=(0, 10), ha='center', va='bottom', color='black')\n",
    "\n",
    "def plot_feature_importance(df, constant_line1=None, constant_line2=None):\n",
    "    # Create a figure and a grid of subplots (2 rows, 1 column)\n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(30, 20), sharex=True)\n",
    "\n",
    "    # Top Subplot\n",
    "    # Plotting F1 Median, Precision, Recall on the primary Y-axis\n",
    "    ax1.plot(df['cum_value_percentage'], df['f1_median'], marker='o', linestyle='--', color='black', label='F1 Median')\n",
    "    ax1.plot(df['cum_value_percentage'], df['precision_median'], marker='^', linestyle='-.', color='g', label='Precision')\n",
    "    ax1.plot(df['cum_value_percentage'], df['recall_median'], marker='v', linestyle=':', color='orange', label='Recall')\n",
    "    \n",
    "    # Add the first constant line\n",
    "    if constant_line1 is not None:\n",
    "        ax1.axhline(y=constant_line1, color='red', linestyle='-', label='without_fs1')\n",
    "    \n",
    "    ax1.set_ylabel('Scores', color='black')\n",
    "    ax1.tick_params('y', colors='black')\n",
    "    ax1.legend(loc='upper left')\n",
    "\n",
    "    # Add Stability to the secondary Y-axis\n",
    "    ax1_2 = ax1.twinx()\n",
    "    ax1_2.plot(df['cum_value_percentage'], df['n_feats_percentage'], marker='D', linestyle='-', color='purple', label='% Features')\n",
    "    ax1_2.set_ylabel('% Features', color='black')\n",
    "    ax1_2.tick_params('y', colors='black')\n",
    "    add_percentage_annotations(ax1_2, df['cum_value_percentage'], df['n_feats_percentage'], df['n_feats_percentage'])\n",
    "    ax1_2.legend(loc='upper right')\n",
    "\n",
    "    # Bottom Subplot\n",
    "    # Bar plot on the primary Y-axis\n",
    "    bars = ax2.bar(df['cum_value_percentage'], df['n_feats'], label='# Features', color='purple')\n",
    "    # add value annotations on top of each bar\n",
    "    for bar, value in zip(bars, df['n_feats']):\n",
    "        ax2.annotate(str(value), xy=(bar.get_x() + bar.get_width() / 2, bar.get_height()),\n",
    "                     ha='center', va='bottom', color='black')\n",
    "    \n",
    "    # Add the second constant line to the second Y-axis\n",
    "    ax2_2 = ax2.twinx()\n",
    "    if constant_line2 is not None:\n",
    "        ax2_2.axhline(y=constant_line2, color='blue', linestyle='-', label='without_fs2')\n",
    "\n",
    "    ax2.set_xlabel('Feature importance by SHAP')\n",
    "    ax2.set_ylabel('# Features')\n",
    "\n",
    "    # Add Stability to the secondary Y-axis\n",
    "    ax2_2.plot(df['cum_value_percentage'], df['model_stab_median'], color='b', label='% Stability model', marker='o', linestyle='--')\n",
    "    ax2_2.plot(df['cum_value_percentage'], df['shap_stab_median'], color='b', label='% Stability model', marker='o', linestyle='--')\n",
    "    ax2_2.set_ylabel('% Stability model', color='b')\n",
    "    ax2_2.tick_params('y', colors='b')\n",
    "    add_percentage_annotations(ax2_2, df['cum_value_percentage'], df['model_stab_median'], df['model_stab_median'])\n",
    "    ax2.legend(loc='upper right')\n",
    "    ax2_2.legend(loc='upper left')\n",
    "\n",
    "    # Set X-axis ticks to the exact values from the 'cum_value_percentage' column for both subplots\n",
    "    plt.xticks(round(df['cum_value_percentage'], 0))\n",
    "\n",
    "    # Display the plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b9a37d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_shap = pd.read_parquet(path_shap)\n",
    "data_shap = data_shap.reset_index()\n",
    "fi_shap_all = pd.read_parquet(path_fi_shap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deabce2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample DataFrame\n",
    "df = pd.merge(data_shap, fi_shap_all, on='n_feats', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "313d0027",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_graph = df[(df.n_estimators==100) & (df.contamination==0.182)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "021732a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the function with the constant lines parameters\n",
    "plot_feature_importance(df_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a4575ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f52ba889",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "def plot_variables(df, variable_configs):\n",
    "    # Grouping by 'n_estimators' and 'contamination'\n",
    "    grouped = df.groupby(['n_estimators', 'contamination'])\n",
    "\n",
    "    # Set up the subplot grid\n",
    "    fig, axes = plt.subplots(nrows=3, ncols=3, figsize=(15, 15), sharex=True, sharey=True)\n",
    "\n",
    "    # Flatten the 2D array of subplots for easy indexing\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    # Initialize variables to store min and max values for setting axis limits\n",
    "    min_n_feats, max_n_feats = float('inf'), float('-inf')\n",
    "    min_variable, max_variable = float('inf'), float('-inf')\n",
    "\n",
    "    for i, (group, data) in enumerate(grouped):\n",
    "        n_estimators, contamination = group\n",
    "\n",
    "        # Plotting on the i-th subplot for each variable in the list\n",
    "        for variable_name, y_axis in variable_configs:\n",
    "            color = plt.cm.viridis(variable_configs.index((variable_name, y_axis)) / len(variable_configs))  # Use a colormap for line colors\n",
    "            if y_axis == 1:\n",
    "                axes[i].plot(data['n_feats'], data[variable_name], label=f'{variable_name}', marker='o', color=color)\n",
    "            elif y_axis == 2:\n",
    "                axes[i].twinx().plot(data['n_feats'], data[variable_name], label=f'{variable_name}', marker='o', color=color)\n",
    "\n",
    "            # Add a vertical line at the maximum value of each variable with the same color\n",
    "            max_variable_value = data[variable_name].max()\n",
    "            max_variable_n_feats = data.loc[data[variable_name].idxmax(), 'n_feats']\n",
    "            axes[i].axvline(x=max_variable_n_feats, color=color, linestyle='--', label=f'Max {variable_name} at {max_variable_n_feats:.2f}')\n",
    "\n",
    "            # Update min and max values for setting axis limits\n",
    "            min_n_feats = min(min_n_feats, data['n_feats'].min())\n",
    "            max_n_feats = max(max_n_feats, data['n_feats'].max())\n",
    "            min_variable = min(min_variable, data[variable_name].min())\n",
    "            max_variable = max(max_variable, data[variable_name].max())\n",
    "\n",
    "        # Set background color for the subplot with the highest value in the first variable\n",
    "        max_variable_name, max_y_axis = variable_configs[0]\n",
    "        max_group = df.loc[df[max_variable_name].idxmax(), ['n_estimators', 'contamination']]\n",
    "        if group[0] == max_group['n_estimators'] and group[1] == max_group['contamination']:\n",
    "            axes[i].set_facecolor('lightblue')\n",
    "\n",
    "        # Customize the subplot\n",
    "        axes[i].set_title(f'n_estimators={n_estimators}, contamination={contamination}')\n",
    "        axes[i].set_xlabel('n_feats')\n",
    "        axes[i].grid(True)\n",
    "\n",
    "    # Set the same scale for the X-axis and Y-axis in all subplots\n",
    "    for ax in axes:\n",
    "        ax.set_xlim(min_n_feats * 0.8, max_n_feats * 1.1)\n",
    "        ax.set_ylim(min_variable * 0.8, max_variable * 1.2)\n",
    "\n",
    "        # Set labels for X-axis and Y-axis\n",
    "        ax.set_xlabel('n_feats')\n",
    "        ax.set_ylabel('Score')\n",
    "\n",
    "    # Add a common legend\n",
    "    lines, labels = axes[0].get_legend_handles_labels()\n",
    "    fig.legend(lines, labels, loc='upper right', bbox_to_anchor=(0.95, 0.95))\n",
    "\n",
    "    # Adjust layout for better spacing\n",
    "    plt.tight_layout(rect=[0, 0, 0.9, 0.95])\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2416b4ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming your DataFrame is named df\n",
    "# You can replace df with your actual DataFrame variable\n",
    "# Specify the list of variables you want to plot (e.g., ['f1_median', 'roc_auc', 'another_variable'])\n",
    "plot_variables(df, [('f1_median', 1), ('shap_stab_median', 2), ('model_stab_median', 2), ('shap_std_median', 2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44df8b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_diffi = pd.read_parquet(path_diffi)\n",
    "data_diffi = data_diffi.reset_index()\n",
    "fi_diffi_all= pd.read_parquet(path_fi_diffi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e249d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample DataFrame\n",
    "df = pd.merge(data_diffi, fi_diffi_all, on='n_feats', how='inner')\n",
    "# df = df[df.contamination == 0.161]\n",
    "\n",
    "# Call the function with the constant lines parameters\n",
    "plot_feature_importance(df, constant_line1=f1_without_fs, constant_line2=stability_without_fs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d21a612",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Assuming 'df_cor' is your DataFrame\n",
    "df_cor = data.copy()\n",
    "df_cor = df_cor.loc[:, ~df_cor.columns.isin(excluded_cols)]\n",
    "\n",
    "# Replace this with your actual DataFrame\n",
    "correlation_matrix = df_cor.corr(method='spearman')\n",
    "\n",
    "# Create a mask to hide the diagonal and the upper triangle of the correlation matrix (since it's symmetric)\n",
    "mask = (correlation_matrix\n",
    "        .mask(np.triu(np.ones(correlation_matrix.shape), k=1).astype(bool))\n",
    "        .abs()\n",
    "        .stack()\n",
    "        .sort_values(ascending=False)\n",
    "        .reset_index()\n",
    "        .rename(columns={0: 'Correlation'})\n",
    "        .query('level_0 != level_1'))  # Exclude pairs where the feature is correlated with itself\n",
    "\n",
    "# Display the top N most correlated pairs (excluding diagonal and self-correlation)\n",
    "N = 10  # Set the number of top pairs you want to display\n",
    "mask.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4c12dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "fi_diffi_df = fi_diffi.copy()\n",
    "fi_shap_df = fi_shap.copy()\n",
    "\n",
    "fi_diffi_df['rank_diffi'] = fi_diffi_df['value'].rank()\n",
    "fi_shap_df['rank_shap'] = fi_shap_df['value'].rank()\n",
    "\n",
    "df = pd.merge(fi_shap_df[['feature', 'rank_shap']], fi_diffi_df[['feature', 'rank_diffi']], on='feature', how='inner')\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Example DataFrame\n",
    "\n",
    "# Scatter plot\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.scatter(df['rank_shap'], df['rank_diffi'], marker='o', color='blue')\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Rank in SHAP')\n",
    "plt.ylabel('Rank in DIFFI')\n",
    "plt.title('Comparison of Feature Order between DataFrames')\n",
    "\n",
    "# Annotate points with feature names\n",
    "for i, feature in enumerate(df['feature']):\n",
    "    plt.annotate(feature, (df['rank_shap'].iloc[i], df['rank_diffi'].iloc[i]), textcoords=\"offset points\", xytext=(5, 5), ha='left')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c396f8d4",
   "metadata": {},
   "source": [
    "### Test best model with fs by SHAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1631669",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.merge(data_shap, fi_shap_all, on='n_feats', how='inner')\n",
    "contamination = df.contamination.unique()[0]\n",
    "selected_var = list(df.sort_values('f1_median', ascending=False)[:1].feat_selected)[0].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "649687bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sort_values('f1_median', ascending=False)[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a574a874",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyper = fs_datasets_hyperparams(dataset_id)\n",
    "hyper['contamination'] = contamination\n",
    "\n",
    "df = train_data[selected_var]\n",
    "\n",
    "X = np.array(df)\n",
    "y = np.array(train_data['y'])\n",
    "feature_names = np.array(df.columns)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "X_train = pd.DataFrame(X_train, columns=feature_names)\n",
    "X_test = pd.DataFrame(X_test, columns=feature_names)\n",
    "\n",
    "model, y_pred, y_scores, y_decision = train_and_predict_isolation_forest(X_train, hyper, excluded_cols=None, random_state=12345)\n",
    "\n",
    "model.fit(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ed55fb6",
   "metadata": {},
   "source": [
    "### SHAP values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a1bf35b",
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = shap.TreeExplainer(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d0e54a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_values = explainer(X_test)\n",
    "\n",
    "df_shap_values = pd.DataFrame(\n",
    "    shap_values.values, columns=feature_names + ['_shap']\n",
    ")\n",
    "\n",
    "df_data = pd.DataFrame(\n",
    "    shap_values.data, columns=feature_names\n",
    ")\n",
    "\n",
    "df = pd.merge(df_data, df_shap_values, left_index=True, right_index=True)\n",
    "\n",
    "df_cor = df.copy()\n",
    "\n",
    "df['y'] = y_test\n",
    "df['y_pred'] = model.predict(X_test)\n",
    "df['y_pred'] = df.apply(def_outlier, axis=1)\n",
    "df['y_score'] = -model.score_samples(X_test)\n",
    "df['score'] = -model.decision_function(X_test)\n",
    "df[\"sum_shap\"] = df[df[feature_names + '_shap'].columns].sum(axis=1)\n",
    "df[\"base_value\"] = df[\"score\"] - df[\"sum_shap\"]\n",
    "\n",
    "anomaly_shap_values = explainer(df[df.y==1][feature_names])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "783ee899",
   "metadata": {},
   "source": [
    "### FI global SHAP "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "476c417e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import shap\n",
    "shap.initjs()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "012c5d67",
   "metadata": {},
   "source": [
    "### Global importance feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b9b08ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.plots.beeswarm(shap_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf55a2bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.plots.beeswarm(anomaly_shap_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f53a4f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "shap.plots.force(shap_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "620c3601",
   "metadata": {},
   "source": [
    "### Global neg importance feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae363c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "anomaly_fi_global = pd.DataFrame(\n",
    "    shap_values.values, columns=feature_names\n",
    ")\n",
    "\n",
    "fi_global = anomaly_fi_global.abs().mean(axis=0)\n",
    "fi_global = fi_global.sort_values(0, ascending=False)\n",
    "fi_global"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af067eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_combined_graphs(df, selected_cols, target):\n",
    "    num_cols = len(selected_cols)\n",
    "\n",
    "    # Set the style of seaborn\n",
    "    sns.set(style=\"whitegrid\")\n",
    "\n",
    "    # Create a figure with multiple subplots\n",
    "    fig, axes = plt.subplots(2, num_cols, figsize=(5 * num_cols, 8))\n",
    "\n",
    "    target_col = 'y' if target == 'real' else 'y_pred'\n",
    "\n",
    "    # Common color constants\n",
    "    green_color = 'green'\n",
    "    red_color = 'red'\n",
    "    blue_color = 'blue'\n",
    "\n",
    "    # Common scatter plot and annotation code\n",
    "    def scatter_and_annotate(data, color, axis):\n",
    "        sns.scatterplot(data=data, x=col, y=f'{col}_shap', color=color, ax=axis, alpha=0.5)\n",
    "        for idx, row in data.iterrows():\n",
    "            axis.text(row[col], row[f'{col}_shap'], row[col], ha='left', size='small', color=color)\n",
    "\n",
    "    for i, col in enumerate(selected_cols):\n",
    "        # Plot 2D Density Plot (Heatmap)\n",
    "        sns.kdeplot(data=df, x=col, y=f'{col}_shap', cmap='Blues', fill=True, ax=axes[0, i])\n",
    "        axes[0, i].set_title(f'2D Density Plot (Heatmap) - {col}')\n",
    "\n",
    "        # Scatter plots and annotations for y=0 and y=1\n",
    "        scatter_and_annotate(df[df[target_col] == 0], green_color, axes[0, i])\n",
    "        scatter_and_annotate(df[df[target_col] == 1], red_color, axes[0, i])\n",
    "\n",
    "        # Store X-axis limits for the first row\n",
    "        x_limits = axes[0, i].get_xlim()\n",
    "\n",
    "        # Density Plot\n",
    "        sns.kdeplot(data=df, x=col, fill=False, color=blue_color, alpha=0.5, ax=axes[1, i])\n",
    "        sns.kdeplot(data=df[df[target_col] == 1], x=col, fill=False, color=red_color, alpha=0.5, ax=axes[1, i])\n",
    "        sns.kdeplot(data=df[df[target_col] == 0], x=col, fill=False, color=green_color, alpha=0.5, ax=axes[1, i])\n",
    "        axes[1, i].set_title(f'Density Plot of {col}')\n",
    "        axes[1, i].set_xlabel(col)\n",
    "        axes[1, i].set_ylabel('Density')\n",
    "\n",
    "        # Set X-axis limits for the second row based on the first row\n",
    "        axes[1, i].set_xlim(x_limits)\n",
    "\n",
    "    # Adjust layout to prevent overlap\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1785381",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_cols = 3\n",
    "selected_cols = list(fi_global[0:n_cols].index)\n",
    "selected_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b0de821",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage with multiple columns\n",
    "plot_combined_graphs(df, selected_cols, 'real')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d0ac701",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_combined_graphs(df, selected_cols, 'pred')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7435e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "anomaly_df = df[(df.y==1)][list(feature_names) + ['y', 'y_pred', 'score', 'y_score', 'base_value', 'sum_shap']]\n",
    "anomaly_df[(anomaly_df.Col95==84.0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f868e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "id_pr = 61\n",
    "var = ['Col7', 'Col107', 'Col119', 'Col95']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7796852b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.index==id_pr][var]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d7dc5cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def local_pr_shap(id_pr, df, feature_columns):\n",
    "    fs = feature_columns.tolist()\n",
    "    fs_shap = feature_columns + '_shap'\n",
    "    fs_shap = fs_shap.tolist()\n",
    "\n",
    "    df_local = df[feature_columns][df.index == id_pr]\n",
    "    base_value = df[\"base_value\"][df.index == id_pr]\n",
    "\n",
    "    shap_values = df[fs_shap][df.index == id_pr]\n",
    "    shap_values = shap_values.values.ravel()\n",
    "\n",
    "    return df_local, shap_values, base_value.iloc[0]\n",
    "\n",
    "\n",
    "def local_shap_table(id_pred, df_local, feature_columns):\n",
    "    \n",
    "    fs = feature_columns.tolist()\n",
    "    fs_shap = feature_columns + '_shap'\n",
    "    fs_shap = fs_shap.tolist()\n",
    "\n",
    "    df_local = df[feature_columns][df.index == id_pr]\n",
    "    base_value = df[\"base_value\"][df.index == id_pr]\n",
    "\n",
    "    shap_values_local = df[fs_shap][df.index == id_pr]\n",
    "\n",
    "    shap_temp = pd.DataFrame(shap_values_local, columns=fs_shap)\n",
    "    shap_temp.columns = shap_temp.columns.str.replace('_shap', '')\n",
    "    values = pd.DataFrame(df_local, columns=fs)\n",
    "\n",
    "    shap_temp = pd.melt(shap_temp, value_vars=df_local.columns)\n",
    "    shap_temp = shap_temp.rename(columns={\"value\": \"shap_value\"})\n",
    "    values = pd.melt(values, value_vars=df_local.columns)\n",
    "\n",
    "    table = pd.merge(\n",
    "        values, shap_temp, how=\"left\", left_on=\"variable\", right_on=\"variable\"\n",
    "    )\n",
    "\n",
    "    return table.sort_values(\"shap_value\", ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b4bc839",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_local, shap_values_local, base_value = local_pr_shap(id_pr, df, feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da51a68f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_local.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30eee712",
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.plots.force(-base_value, -shap_values_local, df_local, matplotlib = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4165213e",
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.plots.waterfall(anomaly_shap_values[10], max_display=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f616b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "table = local_shap_table(id_pr, df_local, feature_names)\n",
    "table.head(n_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5991f416",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_cols = 4\n",
    "selected_cols = list(table[0:n_cols].variable)\n",
    "selected_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a6329f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_combined_graphs(df, selected_cols, 'real')\n",
    "plot_combined_graphs(df, selected_cols, 'pred')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa030c8f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fdcc1d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "factors_to_test = [0.97, 1, 1.03] # Add more factors if needed\n",
    "\n",
    "detail_table, table_out = generate_shap_tables(df, feature_names, explainer, factors_to_test, beta_flavor=1, gamma=1.4, psi=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e32933d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Assuming your DataFrame is named result_tables\n",
    "# You can replace result_tables with your actual DataFrame variable\n",
    "\n",
    "nte = len(factors_to_test)\n",
    "random_stdev = np.sqrt((nte + 1) * (nte - 1) / (12 * nte**2))\n",
    "\n",
    "# Calculate stability_scores_id for every different id\n",
    "result_tables['stability_scores_id'] = result_tables.groupby('id').apply(lambda group: np.mean(np.minimum(1, group['point_stabilities'] / random_stdev))).reset_index(level=0, drop=True)\n",
    "\n",
    "# Select the required columns\n",
    "result = result_tables[['id', 'stability_scores_id']].drop_duplicates().reset_index(drop=True)\n",
    "result = result.dropna()\n",
    "result['stability_scores_id'] = 1 - result['stability_scores_id']\n",
    "# Display the result\n",
    "result.describe()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c02c6cb-54b6-4c76-af0d-27bdcdfe0c35",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Credit card"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2824c805",
   "metadata": {},
   "source": [
    "**Dataset source**: https://www.kaggle.com/mlg-ulb/creditcardfraud\n",
    "\n",
    "**Additional sources:**\n",
    "\n",
    "Andrea Dal Pozzolo, Olivier Caelen, Reid A. Johnson and Gianluca Bontempi. Calibrating Probability with Undersampling for Unbalanced Classification. In Symposium on Computational Intelligence and Data Mining (CIDM), IEEE, 2015\n",
    "\n",
    "Dal Pozzolo, Andrea; Caelen, Olivier; Le Borgne, Yann-Ael; Waterschoot, Serge; Bontempi, Gianluca. Learned lessons in credit card fraud detection from a practitioner perspective, Expert systems with applications,41,10,4915-4928,2014, Pergamon\n",
    "\n",
    "Dal Pozzolo, Andrea; Boracchi, Giacomo; Caelen, Olivier; Alippi, Cesare; Bontempi, Gianluca. Credit card fraud detection: a realistic modeling and a novel learning strategy, IEEE transactions on neural networks and learning systems,29,8,3784-3797,2018,IEEE\n",
    "\n",
    "Dal Pozzolo, Andrea Adaptive Machine learning for credit card fraud detection ULB MLG PhD thesis (supervised by G. Bontempi)\n",
    "\n",
    "Carcillo, Fabrizio; Dal Pozzolo, Andrea; Le Borgne, Yann-Aël; Caelen, Olivier; Mazzer, Yannis; Bontempi, Gianluca. Scarff: a scalable framework for streaming credit card fraud detection with Spark, Information fusion,41, 182-194,2018,Elsevier\n",
    "\n",
    "Carcillo, Fabrizio; Le Borgne, Yann-Aël; Caelen, Olivier; Bontempi, Gianluca. Streaming active learning strategies for real-life credit card fraud detection: assessment and visualization, International Journal of Data Science and Analytics, 5,4,285-300,2018,Springer International Publishing\n",
    "\n",
    "Bertrand Lebichot, Yann-Aël Le Borgne, Liyun He, Frederic Oblé, Gianluca Bontempi Deep-Learning Domain Adaptation Techniques for Credit Cards Fraud Detection, INNSBDDL 2019: Recent Advances in Big Data and Deep Learning, pp 78-88, 2019\n",
    "\n",
    "Fabrizio Carcillo, Yann-Aël Le Borgne, Olivier Caelen, Frederic Oblé, Gianluca Bontempi Combining Unsupervised and Supervised Learning in Credit Card Fraud Detection Information Sciences, 2019\n",
    "\n",
    "Yann-Aël Le Borgne, Gianluca Bontempi Machine Learning for Credit Card Fraud Detection - Practical Handbook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abaad9a8-e22d-40a5-9e2b-1611697c48c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('./creditcard.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f805c4b2-f0cf-4b91-bd69-60bb6acf57d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop(columns = ['Time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c53f52a7-5385-452d-b93e-0526c104f5c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64fd85a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff70cce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.pivot_table(data,\n",
    "             values = 'V1',\n",
    "               index = 'Class', \n",
    "              aggfunc = 'count')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6960a1f-2d2b-4329-907b-57a2e0f7bf01",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### iForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "953dc268",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4975319b-8081-4b77-b31e-e469381b1fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.process_time()\n",
    "\n",
    "clf = IsolationForest(max_samples=256, n_estimators=100)\n",
    "clf.fit(train_data.loc[:, train_data.columns != 'Class'])\n",
    "\n",
    "end = time.process_time()\n",
    "creditcard_iforest_train_time = end - start\n",
    "print(end - start)\n",
    "\n",
    "start = time.process_time()\n",
    "\n",
    "y_pred = clf.predict(train_data.loc[:, train_data.columns != 'Class'])\n",
    "y_scores = clf.score_samples(train_data.loc[:, train_data.columns != 'Class'])\n",
    "\n",
    "end = time.process_time()\n",
    "creditcard_iforest_test_time = end - start\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3386ac14-a5be-42aa-9da2-4257cdb684d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['y_pred'] = y_pred\n",
    "train_data['prediction'] = train_data.apply(def_outlier, axis = 1)\n",
    "train_data['y_scores'] = -y_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3bbe549-7336-47bf-ad6a-0a582f8dce94",
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(train_data['Class'], train_data['prediction'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4da6e334",
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, _ = metrics.roc_curve(train_data['Class'], train_data['y_scores'])\n",
    "creditcard_iforest_auc = metrics.auc(fpr, tpr)\n",
    "metrics.auc(fpr, tpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13cbb789-5108-40d7-a569-7754a04ae2d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "creditcard_iforest_report = classification_report(train_data['Class'], train_data['prediction'], target_names = ['0','1'], output_dict = True)\n",
    "print(classification_report(train_data['Class'], train_data['prediction'], target_names = ['0','1']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f630fd4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(creditcard_iforest_report['1']['precision'])\n",
    "print(creditcard_iforest_report['1']['recall'])\n",
    "print(creditcard_iforest_report['1']['f1-score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1996778",
   "metadata": {},
   "outputs": [],
   "source": [
    "precision, recall, thresholds = precision_recall_curve(train_data['Class'], train_data['y_scores'])\n",
    "creditcard_iforest_auc_precision_recall = metrics.auc(recall, precision)\n",
    "print(creditcard_iforest_auc_precision_recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2dced57-748e-4374-b5f8-85488e5baf9d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## bank"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd041586",
   "metadata": {},
   "source": [
    "**Dataset source**: https://github.com/GuansongPang/ADRepository-Anomaly-detection-datasets/tree/main/categorical%20data\n",
    "\n",
    "Pang, G., Shen, C., Cao, L., & Hengel, A. V. D. (2021). Deep learning for anomaly detection: A review. ACM Computing Surveys (CSUR), 54(2), 1-38."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45331943-15fa-4768-a7b0-de9de07366ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('./bank.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d466cef-6b84-4710-bf4b-4f83fcfa54d7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "199d3f42-c83b-405f-ad36-39998fd67eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.pivot_table(data,\n",
    "             values = 'age',\n",
    "               index = 'class', \n",
    "              aggfunc = 'count')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5467df1-72aa-43e1-8797-e831037a25c8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### iForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e1b196-7c52-4926-b82a-30179b1ba304",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "036775dd-38a8-44f3-9efa-f8c9eac677f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.process_time()\n",
    "\n",
    "clf = IsolationForest(max_samples = 256, n_estimators = 100)\n",
    "clf.fit(train_data.loc[:, train_data.columns != 'class'])\n",
    "\n",
    "end = time.process_time()\n",
    "bank_iforest_train_time = end - start\n",
    "print(end - start)\n",
    "\n",
    "start = time.process_time()\n",
    "\n",
    "y_pred = clf.predict(train_data.loc[:, train_data.columns != 'class'])\n",
    "y_scores = clf.score_samples(train_data.loc[:, train_data.columns != 'class'])\n",
    "end = time.process_time()\n",
    "bank_iforest_test_time = end - start\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5015ce03-ceae-43cc-94ef-0e5035378a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['y_pred'] = y_pred\n",
    "train_data['prediction'] = train_data.apply(def_outlier, axis = 1)\n",
    "train_data['y_scores'] = -y_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94fe90fc-98ab-4d4c-8010-1f46add0841a",
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(train_data['class'], train_data['prediction'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b83ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, _ = metrics.roc_curve(train_data['class'], train_data['y_scores'])\n",
    "bank_iforest_auc = metrics.auc(fpr, tpr)\n",
    "metrics.auc(fpr, tpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3e59324-e3e3-41ff-89e5-723169429098",
   "metadata": {},
   "outputs": [],
   "source": [
    "bank_iforest_report = classification_report(train_data['class'], train_data['prediction'], target_names = ['0','1'], output_dict=True)\n",
    "print(classification_report(train_data['class'], train_data['prediction'], target_names = ['0','1']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b5877e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(bank_iforest_report['1']['precision'])\n",
    "print(bank_iforest_report['1']['recall'])\n",
    "print(bank_iforest_report['1']['f1-score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce4674d",
   "metadata": {},
   "outputs": [],
   "source": [
    "precision, recall, thresholds = precision_recall_curve(train_data['class'], train_data['y_scores'])\n",
    "bank_iforest_auc_precision_recall = metrics.auc(recall, precision)\n",
    "print(bank_iforest_auc_precision_recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e82b4ae2",
   "metadata": {},
   "source": [
    "## Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4946ce00",
   "metadata": {},
   "outputs": [],
   "source": [
    "performance = pd.DataFrame(columns = ['F1 score', 'recall', 'precision', 'AUC', 'AUPRC', \n",
    "                                      'Training time','Inference time','Total time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e03189fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score_iforest = {'arrhythmia':arrhythmia_iforest_report['1']['f1-score'],\n",
    "                       'cardio':cardio_iforest_report['1']['f1-score'], \n",
    "                        'forestcover':forestcover_iforest_report['1']['f1-score'], \n",
    "                       'annthyroid':annthyroid_iforest_report['1']['f1-score'],       \n",
    "                        'creditcard':creditcard_iforest_report['1']['f1-score'], \n",
    "                       'mammography':mammography_iforest_report['1']['f1-score'], \n",
    "                        'shuttle':shuttle_iforest_report['1']['f1-score'], \n",
    "                      'mnist':mnist_iforest_report['1']['f1-score'], \n",
    "                  'vowels':vowels_iforest_report['1']['f1-score'], \n",
    "                  'seismic':seismic_iforest_report['1']['f1-score'], \n",
    "                  'musk':musk_iforest_report['1']['f1-score'], \n",
    "                  'bank':bank_iforest_report['1']['f1-score']}\n",
    "f1_score_iforest_df = pd.DataFrame.from_dict(f1_score_iforest, orient='index', columns = ['F1 score']).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b80fd6ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "recall_iforest = {'arrhythmia':arrhythmia_iforest_report['1']['recall'],\n",
    "                       'cardio':cardio_iforest_report['1']['recall'], \n",
    "                        'forestcover':forestcover_iforest_report['1']['recall'], \n",
    "                       'annthyroid':annthyroid_iforest_report['1']['recall'],       \n",
    "                        'creditcard':creditcard_iforest_report['1']['recall'], \n",
    "                       'mammography':mammography_iforest_report['1']['recall'], \n",
    "                        'shuttle':shuttle_iforest_report['1']['recall'], \n",
    "                      'mnist':mnist_iforest_report['1']['recall'], \n",
    "                  'vowels':vowels_iforest_report['1']['recall'], \n",
    "                  'seismic':seismic_iforest_report['1']['recall'], \n",
    "                  'musk':musk_iforest_report['1']['recall'], \n",
    "                  'bank':bank_iforest_report['1']['recall'], }\n",
    "recall_iforest_df = pd.DataFrame.from_dict(recall_iforest, orient='index', columns = ['Recall']).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a8b8f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_iforest = {'arrhythmia':arrhythmia_iforest_report['1']['precision'],\n",
    "                       'cardio':cardio_iforest_report['1']['precision'], \n",
    "                        'forestcover':forestcover_iforest_report['1']['precision'], \n",
    "                       'annthyroid':annthyroid_iforest_report['1']['precision'],       \n",
    "                        'creditcard':creditcard_iforest_report['1']['precision'], \n",
    "                       'mammography':mammography_iforest_report['1']['precision'], \n",
    "                        'shuttle':shuttle_iforest_report['1']['precision'], \n",
    "                      'mnist':mnist_iforest_report['1']['precision'], \n",
    "                  'vowels':vowels_iforest_report['1']['precision'], \n",
    "                  'seismic':seismic_iforest_report['1']['precision'], \n",
    "                  'musk':musk_iforest_report['1']['precision'], \n",
    "                  'bank':bank_iforest_report['1']['precision'], }\n",
    "precision_iforest_df = pd.DataFrame.from_dict(precision_iforest, orient='index', columns = ['Precision']).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36991998",
   "metadata": {},
   "outputs": [],
   "source": [
    "auc_iforest = {'arrhythmia':arrhythmia_iforest_auc,\n",
    "                       'cardio':cardio_iforest_auc, \n",
    "                        'forestcover':forestcover_iforest_auc, \n",
    "                       'annthyroid':annthyroid_iforest_auc,       \n",
    "                        'creditcard':creditcard_iforest_auc, \n",
    "                       'mammography':mammography_iforest_auc, \n",
    "                        'shuttle':shuttle_iforest_auc, \n",
    "                      'mnist':mnist_iforest_auc, \n",
    "                  'vowels':vowels_iforest_auc, \n",
    "                  'seismic':seismic_iforest_auc, \n",
    "                  'musk':musk_iforest_auc, \n",
    "                  'bank':bank_iforest_auc}\n",
    "auc_iforest_df = pd.DataFrame.from_dict(auc_iforest, orient='index', columns = ['AUC']).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "018bf8ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "auprc_iforest = {'arrhythmia':arrhythmia_iforest_auc_precision_recall,\n",
    "                       'cardio':cardio_iforest_auc_precision_recall, \n",
    "                        'forestcover':forestcover_iforest_auc_precision_recall, \n",
    "                       'annthyroid':annthyroid_iforest_auc_precision_recall,       \n",
    "                        'creditcard':creditcard_iforest_auc_precision_recall, \n",
    "                       'mammography':mammography_iforest_auc_precision_recall, \n",
    "                        'shuttle':shuttle_iforest_auc_precision_recall, \n",
    "                      'mnist':mnist_iforest_auc_precision_recall, \n",
    "                  'vowels':vowels_iforest_auc_precision_recall, \n",
    "                  'seismic':seismic_iforest_auc_precision_recall, \n",
    "                  'musk':musk_iforest_auc_precision_recall, \n",
    "                  'bank':bank_iforest_auc_precision_recall}\n",
    "auprc_iforest_df = pd.DataFrame.from_dict(auprc_iforest, orient='index', columns = ['AUPRC']).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0c81bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_time_iforest = {'arrhythmia':arrhythmia_iforest_train_time,\n",
    "                       'cardio':cardio_iforest_train_time, \n",
    "                        'forestcover':forestcover_iforest_train_time, \n",
    "                       'annthyroid':annthyroid_iforest_train_time,       \n",
    "                        'creditcard': creditcard_iforest_train_time, \n",
    "                       'mammography':mammography_iforest_train_time, \n",
    "                        'shuttle':shuttle_iforest_train_time, \n",
    "                      'mnist':mnist_iforest_train_time, \n",
    "                  'vowels':vowels_iforest_train_time, \n",
    "                  'seismic':seismic_iforest_train_time, \n",
    "                  'musk':musk_iforest_train_time, \n",
    "                  'bank':bank_iforest_train_time}\n",
    "training_time_iforest_df = pd.DataFrame.from_dict(training_time_iforest, orient='index', columns = ['Training time']).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d0427e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_time_iforest = {'arrhythmia':arrhythmia_iforest_test_time,\n",
    "                       'cardio':cardio_iforest_test_time, \n",
    "                        'forestcover':forestcover_iforest_test_time, \n",
    "                       'annthyroid':annthyroid_iforest_test_time,       \n",
    "                        'creditcard':creditcard_iforest_test_time, \n",
    "                       'mammography':mammography_iforest_test_time, \n",
    "                        'shuttle':shuttle_iforest_test_time, \n",
    "                      'mnist':mnist_iforest_test_time, \n",
    "                  'vowels':vowels_iforest_test_time, \n",
    "                  'seismic':seismic_iforest_test_time, \n",
    "                  'musk':musk_iforest_test_time, \n",
    "                  'bank':bank_iforest_test_time}\n",
    "test_time_iforest_df = pd.DataFrame.from_dict(test_time_iforest, orient='index', columns = ['Testing time']).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fd50ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_time_iforest = {'arrhythmia':arrhythmia_iforest_train_time + arrhythmia_iforest_test_time,\n",
    "                       'cardio':cardio_iforest_train_time + cardio_iforest_test_time, \n",
    "                        'forestcover':forestcover_iforest_train_time + forestcover_iforest_test_time, \n",
    "                       'annthyroid':annthyroid_iforest_train_time + annthyroid_iforest_test_time,       \n",
    "                        'creditcard': creditcard_iforest_train_time + creditcard_iforest_test_time, \n",
    "                       'mammography':mammography_iforest_train_time + mammography_iforest_test_time, \n",
    "                        'shuttle':shuttle_iforest_train_time + shuttle_iforest_test_time, \n",
    "                      'mnist':mnist_iforest_train_time + mnist_iforest_test_time, \n",
    "                  'vowels':vowels_iforest_train_time + vowels_iforest_test_time, \n",
    "                  'seismic':seismic_iforest_train_time + seismic_iforest_test_time, \n",
    "                  'musk':musk_iforest_train_time + musk_iforest_test_time, \n",
    "                  'bank':bank_iforest_train_time + bank_iforest_test_time}\n",
    "total_time_iforest_df = pd.DataFrame.from_dict(total_time_iforest, orient='index', columns = ['Total time']).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e48ad877",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.merge(pd.merge(pd.merge(pd.merge(pd.merge(pd.merge(pd.merge(f1_score_iforest_df, recall_iforest_df, how = 'inner'), \n",
    "                                    precision_iforest_df, how ='inner'),\n",
    "         auc_iforest_df, how = 'inner'), auprc_iforest_df, how = 'inner'), training_time_iforest_df, how = 'inner'), \n",
    "         test_time_iforest_df, how = 'inner'),total_time_iforest_df, how = 'inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de50dad8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "common-cpu.m80",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m80"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "c7bb90c4ec68b2a8968b0075ab0b1cb7a78770acf7a7acf2e36e903fa05bac64"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
